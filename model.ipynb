{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c62df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Reading: 1706.03762v7.pdf\n",
      "üìÑ Reading: 1810.04805v2.pdf\n",
      "üìÑ Reading: 2005.14165v4.pdf\n",
      "üìÑ Reading: 2010.11929v2.pdf\n",
      "üìÑ Reading: 2302.13971v1.pdf\n",
      "üìÑ Reading: GenAI_in_Academic_Writing.pdf\n",
      "\n",
      "‚úÖ Total chunks created from all PDFs: 1310\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ‚úÖ Step 1: Extract text from a single PDF file\n",
    "def parse_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# ‚úÖ Step 2: Chunk the text for RAG-style retrieval\n",
    "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# ‚úÖ Step 3: Manually specify PDF paths\n",
    "pdf_paths = [\n",
    "    \"1706.03762v7.pdf\",\n",
    "    \"1810.04805v2.pdf\",\n",
    "    \"2005.14165v4.pdf\",\n",
    "    \"2010.11929v2.pdf\",\n",
    "    \"2302.13971v1.pdf\",\n",
    "    \"GenAI_in_Academic_Writing.pdf\"\n",
    "]\n",
    "\n",
    "# ‚úÖ Step 4: Process PDFs\n",
    "all_chunks = []\n",
    "\n",
    "for pdf_file in pdf_paths:\n",
    "    print(f\"üìÑ Reading: {pdf_file}\")\n",
    "    full_text = parse_pdf(pdf_file)\n",
    "    chunks = chunk_text(full_text)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\n‚úÖ Total chunks created from all PDFs: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa811c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VARDAN\\AppData\\Local\\Temp\\ipykernel_1824\\1512568404.py:9: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "C:\\Users\\VARDAN\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\VARDAN\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VARDAN\\.cache\\huggingface\\hub\\models--BAAI--bge-base-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Step 1: Wrap your chunks as LangChain Documents\n",
    "documents = [Document(page_content=chunk) for chunk in all_chunks]\n",
    "\n",
    "# Step 2: Use BAAI/bge-base-en-v1.5 embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-base-en-v1.5\",\n",
    "    model_kwargs={\"device\": \"cpu\"},  # or \"cuda\" if you have a GPU\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # recommended for bge models\n",
    ")\n",
    "\n",
    "# Step 3: Build FAISS index from documents\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Step 4: Save FAISS index\n",
    "vector_store.save_local(\"faiss_store_bge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6d0161",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 5,\n",
    "        \"fetch_k\": 15,\n",
    "        \"lambda_mult\": 0.7\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e124b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Queries: ['how do rainbows occur', 'what creates a rainbow in the sky', 'formation of rainbows explained']\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ‚úÖ Instantiate Groq LLaMA3\n",
    "qlm = ChatGroq(\n",
    "    model_name=\"llama3-70b-8192\"\n",
    ")\n",
    "\n",
    "# ‚úÖ Simplified Prompt (good for most LLMs, including LLaMA3)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "Rephrase the following question into 3 different, yet related, search queries. List them without any explanation or numbering.\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ Create chain using qlm\n",
    "multi_query_chain = LLMChain(llm=qlm, prompt=prompt)\n",
    "\n",
    "# ‚úÖ Function to generate multiple queries\n",
    "def generate_multi_queries_fn(user_query: str):\n",
    "    output = multi_query_chain.run(user_query)\n",
    "    queries = [line.strip() for line in output.strip().split(\"\\n\") if line.strip()]\n",
    "    return queries[:3]\n",
    "\n",
    "# ‚úÖ Test run\n",
    "queries = generate_multi_queries_fn(\"What causes rainbows to form?\")\n",
    "print(\"Generated Queries:\", queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9a1d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def multi_query_retrieve_fn(user_query: str, top_n: int = 5):\n",
    "    # Step 1: Generate reformulated search queries\n",
    "    queries = generate_multi_queries_fn(user_query)\n",
    "\n",
    "    # Step 2: Initialize list of all retrieved documents\n",
    "    all_docs = []\n",
    "\n",
    "    # Step 3: Retrieve from original query\n",
    "    original_docs = retriever.get_relevant_documents(user_query)\n",
    "    all_docs.extend(original_docs)\n",
    "\n",
    "    # Step 4: Retrieve for each reformulated query\n",
    "    for query in queries:\n",
    "        docs = retriever.get_relevant_documents(query)\n",
    "        all_docs.extend(docs)\n",
    "\n",
    "    # Step 5: Deduplicate based on document content\n",
    "    unique_docs = list({doc.page_content: doc for doc in all_docs}.values())\n",
    "\n",
    "    # Step 6: Return top-N results\n",
    "    return unique_docs[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d370f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "104e53db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough,RunnableLambda\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"question\": RunnablePassthrough(),\n",
    "    \"context\": RunnablePassthrough() | multi_query_retrieve_fn | RunnableLambda(format_docs)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87cf39d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VARDAN\\AppData\\Local\\Temp\\ipykernel_1824\\2205979486.py:11: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  original_docs = retriever.get_relevant_documents(user_query)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What Is transformer',\n",
       " 'context': 'language modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\n\\nscribed in Vaswani et al. (2017) and released in\\nthe tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as ‚ÄúThe Annotated Transformer.‚Äù2\\nIn this work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\n\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\n\\n2020).\\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\\nTransformer directly to images, with the fewest possible modiÔ¨Åcations. To do so, we split an image\\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\\nthe model on image classiÔ¨Åcation in supervised fashion.\\n\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_chain.invoke('What Is transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b75eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a knowledgeable AI assistant tasked with answering questions strictly based on the provided context.\n",
    "\n",
    "Instructions:\n",
    "- Use only the context to construct your answer.\n",
    "- If the context is insufficient, say \"The provided context does not contain enough information.\"\n",
    "- Be concise, factual, and avoid speculation.\n",
    "- Do not mention the existence of context in your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "030ccf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " The Transformer architecture is a type of neural network introduced in 2017 by Vaswani et al. in the paper \"Attention is All You Need\". It's primarily designed for sequence-to-sequence tasks, such as machine translation, but has since been widely adopted in other natural language processing (NLP) applications.\n",
      "\n",
      "The Transformer architecture is based on self-attention mechanisms, which allow the model to weigh the importance of different input elements relative to each other. This is different from traditional recurrent neural networks (RNNs), which process sequences sequentially and have recurrence connections that allow them to capture long-range dependencies.\n",
      "\n",
      "The Transformer model consists of an encoder and a decoder. The encoder takes in a sequence of tokens (e.g., words or characters) and outputs a continuous representation of the input sequence. The decoder generates the output sequence, one token at a time, based on the encoder's output and self-attention mechanisms.\n",
      "\n",
      "The key innovations of the Transformer architecture are:\n",
      "\n",
      "* Self-attention mechanisms, which allow the model to attend to different parts of the input sequence simultaneously and weigh their importance.\n",
      "* The use of multi-head attention, which allows the model to jointly attend to information from different representation subspaces.\n",
      "* The elimination of recurrence and convolution, which makes the model parallelizable and more efficient.\n",
      "\n",
      "Overall, the Transformer architecture has revolutionized the field of NLP, achieving state-of-the-art results in many tasks and enabling the development of more powerful and efficient language models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables (make sure GROQ_API_KEY is set in your .env file)\n",
    "load_dotenv()\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama3-70b-8192\",  # ‚úÖ Best model from Groq\n",
    "    temperature=0.2  # Optional: keeps responses factual and deterministic\n",
    ")\n",
    "\n",
    "# Optional test prompt (for sanity check)\n",
    "test_prompt = \"Explain the transformer architecture in 200 words.\"\n",
    "response = llm.invoke(test_prompt)\n",
    "print(\"Response:\\n\", response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7289c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Define the final RAG chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fae1ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = parallel_chain | rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2a5155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT is a pre-trained deep bidirectional representation model designed to pre-train on unlabeled text by jointly conditioning on both left and right context in all layers.\n"
     ]
    }
   ],
   "source": [
    "response = final_chain.invoke(\"What is bert?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20895b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Question: What is a Transformer in deep learning?\n",
      "üß† Predicted Answer: A Transformer is a transduction model that relies entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.\n",
      "‚úÖ Reference Answer: A Transformer is a neural network architecture based on self-attention mechanisms that allows models to weigh the importance of different parts of the input data dynamically.\n",
      "üîó Cosine Similarity: 0.8251\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: How does self-attention work in Transformers?\n",
      "üß† Predicted Answer: In the Transformer, self-attention is used in three different ways, including \"encoder-decoder attention\" layers, where the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder, allowing every position in the decoder to attend over all positions in the input sequence.\n",
      "‚úÖ Reference Answer: Self-attention allows a model to focus on different positions of the input sequence to capture contextual relationships by computing attention scores between all token pairs.\n",
      "üîó Cosine Similarity: 0.5682\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: What is the role of positional encoding in Transformers?\n",
      "üß† Predicted Answer: To inject information about the relative or absolute position of the tokens in the sequence, allowing the model to make use of the order of the sequence.\n",
      "‚úÖ Reference Answer: Positional encoding provides the model with information about the position of tokens in a sequence, which is essential since Transformers lack recurrence.\n",
      "üîó Cosine Similarity: 0.6685\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: Define encoder and decoder in Transformer architecture.\n",
      "üß† Predicted Answer: The encoder is composed of a stack of N = 6 identical layers, each having two sub-layers. The decoder is also composed of a stack of layers, but the exact number of layers is not specified in the provided context.\n",
      "‚úÖ Reference Answer: The encoder processes the input sequence to generate hidden representations, while the decoder uses these representations to produce the output sequence step-by-step.\n",
      "üîó Cosine Similarity: 0.6240\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: What is Generative AI?\n",
      "üß† Predicted Answer: The provided context does not contain a direct definition of \"Generative AI\". However, based on the context, it can be inferred that Generative AI refers to a type of artificial intelligence that is capable of generating content, such as academic writing, news articles, and possibly other forms of content, using large language models.\n",
      "‚úÖ Reference Answer: Generative AI refers to AI systems that can create new content, such as text, images, or music, often using models like GPT, DALL¬∑E, or diffusion models.\n",
      "üîó Cosine Similarity: 0.8339\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: How is GPT different from BERT?\n",
      "üß† Predicted Answer: GPT is different from BERT in the following ways:\n",
      "\n",
      "* GPT is trained on the BooksCorpus (800M words), while BERT is trained on the BooksCorpus (800M words) and Wikipedia (2,500M words).\n",
      "* GPT uses a sentence separator ([SEP]) and classifier token ([CLS]) which are only introduced at fine-tuning time, whereas BERT learns [SEP], [CLS], and sentence A/B embeddings during pre-training.\n",
      "* GPT was trained for 1M steps with a batch size of 32,000 words, while BERT was trained for 1M steps with a batch size of 128,000 words.\n",
      "* GPT uses a left-to-right Transformer, whereas BERT uses a bidirectional Transformer.\n",
      "‚úÖ Reference Answer: GPT is a unidirectional, autoregressive language model designed for generation tasks, while BERT is bidirectional and primarily used for understanding tasks like classification.\n",
      "üîó Cosine Similarity: 0.8644\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: What are large language models (LLMs)?\n",
      "üß† Predicted Answer: Large language models (LLMs) are foundation language models trained on massive corpora of texts, capable of performing new tasks from textual instructions or from a few examples.\n",
      "‚úÖ Reference Answer: Large language models are neural networks trained on massive corpora to understand and generate human-like language based on statistical patterns.\n",
      "üîó Cosine Similarity: 0.6330\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: What is fine-tuning in Generative AI?\n",
      "üß† Predicted Answer: Fine-tuning in Generative AI is an approach that introduces minimal task-specific parameters and is trained on downstream tasks by simply fine-tuning all pre-trained parameters.\n",
      "‚úÖ Reference Answer: Fine-tuning involves taking a pre-trained generative model and adapting it to a specific domain or task by training it on a smaller, specialized dataset.\n",
      "üîó Cosine Similarity: 0.8331\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: Explain prompt engineering in the context of LLMs.\n",
      "üß† Predicted Answer: The provided context does not contain enough information to explain prompt engineering in the context of LLMs.\n",
      "‚úÖ Reference Answer: Prompt engineering is the practice of designing input prompts to guide the output behavior of a language model effectively for specific tasks.\n",
      "üîó Cosine Similarity: 0.4363\n",
      "----------------------------------------------------------------------------------------------------\n",
      "üîπ Question: What are attention heads in Transformers?\n",
      "üß† Predicted Answer: Attention heads in Transformers are learned to perform different tasks, as exhibited by their behavior related to the structure of the sentence.\n",
      "‚úÖ Reference Answer: Attention heads are components within multi-head attention that allow the model to attend to information from different representation subspaces at different positions.\n",
      "üîó Cosine Similarity: 0.5898\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load models\n",
    "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# ‚úÖ Your LangChain chain\n",
    "# Assuming final_chain is already defined as your LangChain chain\n",
    "# from your previous steps: final_chain = LLMChain(prompt=..., llm=..., etc.)\n",
    "\n",
    "# ‚úÖ Transformers & Generative AI QA Dataset\n",
    "qa_dataset = [\n",
    "    (\"What is a Transformer in deep learning?\",\n",
    "     \"A Transformer is a neural network architecture based on self-attention mechanisms that allows models to weigh the importance of different parts of the input data dynamically.\"),\n",
    "\n",
    "    (\"How does self-attention work in Transformers?\",\n",
    "     \"Self-attention allows a model to focus on different positions of the input sequence to capture contextual relationships by computing attention scores between all token pairs.\"),\n",
    "\n",
    "    (\"What is the role of positional encoding in Transformers?\",\n",
    "     \"Positional encoding provides the model with information about the position of tokens in a sequence, which is essential since Transformers lack recurrence.\"),\n",
    "\n",
    "    (\"Define encoder and decoder in Transformer architecture.\",\n",
    "     \"The encoder processes the input sequence to generate hidden representations, while the decoder uses these representations to produce the output sequence step-by-step.\"),\n",
    "\n",
    "    (\"What is Generative AI?\",\n",
    "     \"Generative AI refers to AI systems that can create new content, such as text, images, or music, often using models like GPT, DALL¬∑E, or diffusion models.\"),\n",
    "\n",
    "    (\"How is GPT different from BERT?\",\n",
    "     \"GPT is a unidirectional, autoregressive language model designed for generation tasks, while BERT is bidirectional and primarily used for understanding tasks like classification.\"),\n",
    "\n",
    "    (\"What are large language models (LLMs)?\",\n",
    "     \"Large language models are neural networks trained on massive corpora to understand and generate human-like language based on statistical patterns.\"),\n",
    "\n",
    "    (\"What is fine-tuning in Generative AI?\",\n",
    "     \"Fine-tuning involves taking a pre-trained generative model and adapting it to a specific domain or task by training it on a smaller, specialized dataset.\"),\n",
    "\n",
    "    (\"Explain prompt engineering in the context of LLMs.\",\n",
    "     \"Prompt engineering is the practice of designing input prompts to guide the output behavior of a language model effectively for specific tasks.\"),\n",
    "\n",
    "    (\"What are attention heads in Transformers?\",\n",
    "     \"Attention heads are components within multi-head attention that allow the model to attend to information from different representation subspaces at different positions.\")\n",
    "]\n",
    "\n",
    "# ‚úÖ Evaluate your chain\n",
    "for question, reference_answer in qa_dataset:\n",
    "    # Run your chain (LangChain Runnable)\n",
    "    predicted_answer = final_chain.invoke(question)\n",
    "\n",
    "    # Compute Sentence Similarity\n",
    "    embeddings = similarity_model.encode([predicted_answer, reference_answer], convert_to_tensor=True)\n",
    "    cosine_sim = util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "\n",
    "\n",
    "    # Print Results\n",
    "    print(f\"üîπ Question: {question}\")\n",
    "    print(f\"üß† Predicted Answer: {predicted_answer}\")\n",
    "    print(f\"‚úÖ Reference Answer: {reference_answer}\")\n",
    "    print(f\"üîó Cosine Similarity: {cosine_sim:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
